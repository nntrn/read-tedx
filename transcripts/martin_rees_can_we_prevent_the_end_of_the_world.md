---
title: Can we prevent the end of the world?
speaker: Martin Rees
description: >-
 A post-apocalyptic Earth, emptied of humans, seems like the stuff of science
 fiction TV and movies. But in this short, surprising talk, Lord Martin Rees asks
 us to think about our real existential risks — natural and human-made threats
 that could wipe out humanity. As a concerned member of the human race, he asks:
 What's the worst thing that could possibly happen?
date: 2014-03-13
tags: ["future","humanity","science"]
slug: martin_rees_can_we_prevent_the_end_of_the_world
---

Ten years ago, I wrote a book which I entitled "Our Final Century?" Question mark. My
publishers cut out the question mark. 

The American publishers changed our title to "Our Final Hour." Americans like instant
gratification and the reverse. 

And my theme was this: Our Earth has existed for 45 million centuries, but this one is
special — it's the first where one species, ours, has the planet's future in its hands.
Over nearly all of Earth's history, threats have come from nature — disease, earthquakes,
asteroids and so forth — but from now on, the worst dangers come from us. And it's now not
just the nuclear threat; in our interconnected world, network breakdowns can cascade
globally; air travel can spread pandemics worldwide within days; and social media can
spread panic and rumor literally at the speed of light. We fret too much about minor
hazards — improbable air crashes, carcinogens in food, low radiation doses, and so forth —
but we and our political masters are in denial about catastrophic scenarios. The worst
have thankfully not yet happened. Indeed, they probably won't. But if an event is
potentially devastating, it's worth paying a substantial premium to safeguard against it,
even if it's unlikely, just as we take out fire insurance on our house. And as science
offers greater power and promise, the downside gets scarier too.

We get ever more vulnerable. Within a few decades, millions will have the capability to
misuse rapidly advancing biotech, just as they misuse cybertech today. Freeman Dyson, in a
TED Talk, foresaw that children will design and create new organisms just as routinely as
his generation played with chemistry sets. Well, this may be on the science fiction
fringe, but were even part of his scenario to come about, our ecology and even our species
would surely not survive long unscathed. For instance, there are some eco-extremists who
think that it would be better for the planet, for Gaia, if there were far fewer humans.
What happens when such people have mastered synthetic biology techniques that will be
widespread by 2050? And by then, other science fiction nightmares may transition to
reality: dumb robots going rogue, or a network that develops a mind of its own threatens
us all. Well, can we guard against such risks by regulation?

We must surely try, but these enterprises are so competitive, so globalized, and so driven
by commercial pressure, that anything that can be done will be done somewhere, whatever
the regulations say. It's like the drug laws — we try to regulate, but can't. And the
global village will have its village idiots, and they'll have a global range. So as I said
in my book, we'll have a bumpy ride through this century. There may be setbacks to our
society — indeed, a 50 percent chance of a severe setback. But are there conceivable
events that could be even worse, events that could snuff out all life? When a new particle
accelerator came online, some people anxiously asked, could it destroy the Earth or, even
worse, rip apart the fabric of space? Well luckily, reassurance could be offered. I and
others pointed out that nature has done the same experiments zillions of times already,
via cosmic ray collisions.

But scientists should surely be precautionary about experiments that generate conditions
without precedent in the natural world. Biologists should avoid release of potentially
devastating genetically modified pathogens. And by the way, our special aversion to the
risk of truly existential disasters depends on a philosophical and ethical question, and
it's this: Consider two scenarios. Scenario A wipes out 90 percent of humanity. Scenario B
wipes out 100 percent. How much worse is B than A? Some would say 10 percent worse. The
body count is 10 percent higher. But I claim that B is incomparably worse. As an
astronomer, I can't believe that humans are the end of the story. It is five billion years
before the sun flares up, and the universe may go on forever, so post-human evolution,
here on Earth and far beyond, could be as prolonged as the Darwinian process that's led to
us, and even more wonderful.

And indeed, future evolution will happen much faster, on a technological timescale, not a
natural selection timescale. So we surely, in view of those immense stakes, shouldn't
accept even a one in a billion risk that human extinction would foreclose this immense
potential. Some scenarios that have been envisaged may indeed be science fiction, but
others may be disquietingly real. It's an important maxim that the unfamiliar is not the
same as the improbable, and in fact, that's why we at Cambridge University are setting up
a center to study how to mitigate these existential risks. It seems it's worthwhile just
for a few people to think about these potential disasters. And we need all the help we can
get from others, because we are stewards of a precious pale blue dot in a vast cosmos, a
planet with 50 million centuries ahead of it. And so let's not jeopardize that future. And
I'd like to finish with a quote from a great scientist called Peter Medawar.

I quote, "The bells that toll for mankind are like the bells of Alpine cattle. They are
attached to our own necks, and it must be our fault if they do not make a tuneful and
melodious sound."Thank you very much.

<!--
ad_duration=3.33
event="TED2014"
external_start_time=0
intro_duration=11.82
is_subtitle_required="False"
is_talk_featured="True"
language="en"
language_swap="False"
native_language="en"
number_of_related_talks=6
number_of_speakers=1
number_of_subtitled_videos=33
number_of_tags=3
number_of_talk_download_languages=33
number_of_talk_more_resources=0
number_of_talk_recommendations=0
number_of_talks_take_actions=0
post_ad_duration=0.83
published_timestamp="2014-08-25 15:08:10"
recording_date="2014-03-13"
speaker_description="Astrophysicist"
speaker_id=44
speaker_is_published=1
speaker_name="Martin Rees"
talk_name="Can we prevent the end of the world?"
talks_tags=["future","humanity","science"]
url_audio="https://download.ted.com/talks/SirMartinRees_2014U.mp3?apikey=acme-roadrunner"
url_photo_speaker="https://pe.tedcdn.com/images/ted/1330_253x190.jpg"
url_photo_talk="https://pe.tedcdn.com/images/ted/d3b8fd408c1f2576d86a6d781da0dfd768d0cda4_2400x1800.jpg"
url_webpage="https://www.ted.com/talks/martin_rees_can_we_prevent_the_end_of_the_world"
video_type_name="TED Stage Talk"
-->