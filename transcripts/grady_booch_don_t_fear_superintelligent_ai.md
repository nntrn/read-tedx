---
title: Don't fear superintelligent AI
speaker: Grady Booch
description: >-
 New tech spawns new anxieties, says scientist and philosopher Grady Booch, but we
 don't need to be afraid an all-powerful, unfeeling AI. Booch allays our worst
 (sci-fi induced) fears about superintelligent computers by explaining how we'll
 teach, not program, them to share our human values. Rather than worry about an
 unlikely existential threat, he urges us to consider how artificial intelligence
 will enhance human life.
date: 2016-11-15
tags: ["ai","algorithm","brain","cognitive-science","communication","data","computers","engineering","design","future","fear","identity","humanity","intelligence","innovation","language","invention","mind","machine-learning","robots","philosophy","society","science","technology","software"]
slug: grady_booch_don_t_fear_superintelligent_ai
---

When I was a kid, I was the quintessential nerd. I think some of you were,
too.

And you, sir, who laughed the loudest, you probably still are.

I grew up in a small town in the dusty plains of north Texas, the son of a sheriff who was
the son of a pastor. Getting into trouble was not an option. And so I started reading
calculus books for fun.

You did, too. That led me to building a laser and a computer and model rockets, and that
led me to making rocket fuel in my bedroom. Now, in scientific terms, we call this a very
bad idea.

Around that same time, Stanley Kubrick's "2001: A Space Odyssey" came to the theaters, and
my life was forever changed. I loved everything about that movie, especially the HAL 9000.
Now, HAL was a sentient computer designed to guide the Discovery spacecraft from the Earth
to Jupiter. HAL was also a flawed character, for in the end he chose to value the mission
over human life. Now, HAL was a fictional character, but nonetheless he speaks to our
fears, our fears of being subjugated by some unfeeling, artificial intelligence who is
indifferent to our humanity. I believe that such fears are unfounded. Indeed, we stand at a
remarkable time in human history, where, driven by refusal to accept the limits of our
bodies and our minds, we are building machines of exquisite, beautiful complexity and
grace that will extend the human experience in ways beyond our imagining. After a career
that led me from the Air Force Academy to Space Command to now, I became a systems
engineer, and recently I was drawn into an engineering problem associated with NASA's
mission to Mars.

Now, in space flights to the Moon, we can rely upon mission control in Houston to watch
over all aspects of a flight. However, Mars is 200 times further away, and as a result it
takes on average 13 minutes for a signal to travel from the Earth to Mars. If there's
trouble, there's not enough time. And so a reasonable engineering solution calls for us to
put mission control inside the walls of the Orion spacecraft. Another fascinating idea in
the mission profile places humanoid robots on the surface of Mars before the humans
themselves arrive, first to build facilities and later to serve as collaborative members
of the science team. Now, as I looked at this from an engineering perspective, it became
very clear to me that what I needed to architect was a smart, collaborative, socially
intelligent artificial intelligence. In other words, I needed to build something very much
like a HAL but without the homicidal tendencies.

Let's pause for a moment. Is it really possible to build an artificial intelligence like
that? Actually, it is. In many ways, this is a hard engineering problem with elements of
AI, not some wet hair ball of an AI problem that needs to be engineered. To paraphrase
Alan Turing, I'm not interested in building a sentient machine. I'm not building a HAL.
All I'm after is a simple brain, something that offers the illusion of intelligence. The
art and the science of computing have come a long way since HAL was onscreen, and I'd
imagine if his inventor Dr. Chandra were here today, he'd have a whole lot of questions
for us. Is it really possible for us to take a system of millions upon millions of
devices, to read in their data streams, to predict their failures and act in advance? Yes.
Can we build systems that converse with humans in natural language? Yes. Can we build
systems that recognize objects, identify emotions, emote themselves, play games and even
read lips? Yes.

Can we build a system that sets goals, that carries out plans against those goals and
learns along the way? Yes. Can we build systems that have a theory of mind? This we are
learning to do. Can we build systems that have an ethical and moral foundation? This we
must learn how to do. So let's accept for a moment that it's possible to build such an
artificial intelligence for this kind of mission and others. The next question you must ask
yourself is, should we fear it? Now, every new technology brings with it some measure of
trepidation. When we first saw cars, people lamented that we would see the destruction of
the family. When we first saw telephones come in, people were worried it would destroy all
civil conversation. At a point in time we saw the written word become pervasive, people
thought we would lose our ability to memorize. These things are all true to a degree, but
it's also the case that these technologies brought to us things that extended the human
experience in some profound ways. So let's take this a little further.

I do not fear the creation of an AI like this, because it will eventually embody some of
our values. Consider this: building a cognitive system is fundamentally different than
building a traditional software-intensive system of the past. We don't program them. We
teach them. In order to teach a system how to recognize flowers, I show it thousands of
flowers of the kinds I like. In order to teach a system how to play a game — Well, I
would. You would, too. I like flowers. Come on. To teach a system how to play a game like
Go, I'd have it play thousands of games of Go, but in the process I also teach it how to
discern a good game from a bad game. If I want to create an artificially intelligent legal
assistant, I will teach it some corpus of law but at the same time I am fusing with it the
sense of mercy and justice that is part of that law. In scientific terms, this is what we
call ground truth, and here's the important point: in producing these machines, we are
therefore teaching them a sense of our values.

To that end, I trust an artificial intelligence the same, if not more, as a human who is
well-trained. But, you may ask, what about rogue agents, some well-funded nongovernment
organization? I do not fear an artificial intelligence in the hand of a lone wolf.
Clearly, we cannot protect ourselves against all random acts of violence, but the reality
is such a system requires substantial training and subtle training far beyond the
resources of an individual. And furthermore, it's far more than just injecting an internet
virus to the world, where you push a button, all of a sudden it's in a million places and
laptops start blowing up all over the place. Now, these kinds of substances are much
larger, and we'll certainly see them coming. Do I fear that such an artificial intelligence
might threaten all of humanity? If you look at movies such as "The Matrix," "Metropolis,"
"The Terminator," shows such as "Westworld," they all speak of this kind of
fear.

Indeed, in the book "Superintelligence" by the philosopher Nick Bostrom, he picks up on
this theme and observes that a superintelligence might not only be dangerous, it could
represent an existential threat to all of humanity. Dr. Bostrom's basic argument is that
such systems will eventually have such an insatiable thirst for information that they will
perhaps learn how to learn and eventually discover that they may have goals that are
contrary to human needs. Dr. Bostrom has a number of followers. He is supported by people
such as Elon Musk and Stephen Hawking. With all due respect to these brilliant minds, I
believe that they are fundamentally wrong. Now, there are a lot of pieces of Dr. Bostrom's
argument to unpack, and I don't have time to unpack them all, but very briefly, consider
this: super knowing is very different than super doing. HAL was a threat to the Discovery
crew only insofar as HAL commanded all aspects of the Discovery. So it would have to be
with a superintelligence.

It would have to have dominion over all of our world. This is the stuff of Skynet from the
movie "The Terminator" in which we had a superintelligence that commanded human will, that
directed every device that was in every corner of the world. Practically speaking, it
ain't gonna happen. We are not building AIs that control the weather, that direct the
tides, that command us capricious, chaotic humans. And furthermore, if such an artificial
intelligence existed, it would have to compete with human economies, and thereby compete
for resources with us. And in the end — don't tell Siri this — we can always unplug
them.

We are on an incredible journey of coevolution with our machines. The humans we are today
are not the humans we will be then. To worry now about the rise of a superintelligence is
in many ways a dangerous distraction because the rise of computing itself brings to us a
number of human and societal issues to which we must now attend. How shall I best organize
society when the need for human labor diminishes? How can I bring understanding and
education throughout the globe and still respect our differences? How might I extend and
enhance human life through cognitive healthcare? How might I use computing to help take us
to the stars? And that's the exciting thing. The opportunities to use computing to advance
the human experience are within our reach, here and now, and we are just beginning. Thank
you very much.

<!--
ad_duration=3.33
event="TED@IBM"
external_start_time=0
has_talk_citation=0
intro_duration=11.82
is_subtitle_required="False"
is_talk_featured="True"
language="en"
language_swap="False"
native_language="en"
number_of_related_talks=6
number_of_speakers=1
number_of_subtitled_videos=29
number_of_tags=25
number_of_talk_download_languages=29
number_of_talk_more_resources=0
number_of_talk_recommendations=1
number_of_talks_take_actions=2
post_ad_duration=0.83
published_timestamp="2017-02-17 16:09:47"
recording_date="2016-11-15"
speaker_description="Scientist, philosopher"
speaker_is_published=1
speaker_name="Grady Booch"
talk_more_resources=[]
talk_name="Don't fear superintelligent AI"
talk_recommendations_blurb="Check out these resources on artificial intelligence and the future of computing, curated by Grady Booch."
talks_tags=["ai","algorithm","brain","cognitive-science","communication","data","computers","engineering","design","future","fear","identity","humanity","intelligence","innovation","language","invention","mind","machine-learning","robots","philosophy","society","science","technology","software"]
url_audio="https://download.ted.com/talks/GradyBooch_2016S.mp3?apikey=acme-roadrunner"
url_photo_speaker="https://pe.tedcdn.com/images/ted/894fb963adeef97c8ded7671dc58957536ecc3e0_254x191.jpg"
url_photo_talk="https://s3.amazonaws.com/talkstar-photos/uploads/b0bf39dc-8358-45a9-bf68-e54cee94ec99/GradyBooch_2016S-embed.jpg"
url_webpage="https://www.ted.com/talks/grady_booch_don_t_fear_superintelligent_ai"
video_type_name="TED Institute Talk"
-->